# Step-by-Step Demo Guide for Professor

## Complete Walkthrough: From Code to Results

This guide tells you exactly what to run and what to say at each step.

---

## ðŸ“‹ Pre-Demo Checklist

Before presenting, ensure:

- [ ] Python 3.8+ installed
- [ ] Virtual environment activated
- [ ] Dependencies installed (`pip install -r requirements.txt`)
- [ ] Pre-trained model exists in `outputs/experiment_v1/`
- [ ] Graphs already generated in `outputs/experiment_v1/graphs/`

---

## ðŸŽ¬ The Demo Flow

### Step 1: Show Project Structure (30 seconds)

**Open terminal and run:**

```powershell
cd "c:\Users\khobr\OneDrive\Desktop\MARL JAMMER"
tree /F src
```

**What to say:**

> "The project is organized into modular components:
>
> - `physics/` contains the RF propagation and graph theory models
> - `environment/` has the Gym-compatible simulation
> - `agents/` has the neural networks and PPO implementation
> - Clean separation of concerns following software engineering principles."

---

### Step 2: Show the Configuration (30 seconds)

**Open `src/config.py` and show:**

```python
# These are the key hyperparameters:
num_jammers = 4          # M = 4 jammer drones
num_enemies = 10         # N = 10 enemy swarm
total_timesteps = 200000 # Training steps
rollout_size = 2048      # Steps collected before update
learning_rate = 1e-4     # Conservative for stability
```

**What to say:**

> "The configuration is centralized. We have 4 jammer drones versus 10 enemy nodes.
> The training uses 200K timesteps with PPO and 2048-step rollouts.
> These values were tuned for stable learning - not too fast, not too slow."

---

### Step 3: Execute a Quick Training Demo (5 minutes)

**Run a SHORT training (just to show it works):**

```powershell
cd "c:\Users\khobr\OneDrive\Desktop\MARL JAMMER"
python -m src.train --total_timesteps 5000 --experiment_name demo_live
```

**What happens:**

```
[INFO] Starting training with config:
       num_jammers=4, num_enemies=10, total_timesteps=5000
Step 100 | Reward: 13.5 | Î»â‚‚: 89.2% | KL: 0.012
Step 200 | Reward: 25.7 | Î»â‚‚: 82.4% | KL: 0.018
...
```

**What to say:**

> "Watch how the reward is increasing and Î»â‚‚ is decreasing.
> Î»â‚‚ is the Fiedler value - it measures enemy swarm connectivity.
> Our agents are learning to disrupt their communication graph.
> This is just a quick 5000-step demo; the full training is 200K steps."

---

### Step 4: Load Pre-Trained Model & Evaluate (2 minutes)

**Run evaluation with pre-trained weights:**

```powershell
python -m src.evaluate --experiment_name experiment_v1 --num_episodes 10
```

**Expected output:**

```
Loading trained model from: outputs/experiment_v1/
Episode 1/10 | Total Reward: 84.2 | Final Î»â‚‚: 12.3% | Steps: 100
Episode 2/10 | Total Reward: 79.6 | Final Î»â‚‚: 15.1% | Steps: 100
...
Average Reward: 82.4 Â± 3.2
Average Î»â‚‚ Reduction: 36.3% â†’ 12.8%
```

**What to say:**

> "This is the pre-trained model from 200K steps of training.
> It consistently achieves 36% Î»â‚‚ reduction - that means it's reducing
> the enemy swarm's coordination capability by over a third.
> The reward is stable around 82, showing converged policy."

---

### Step 5: Show the Training Graphs (3 minutes)

**Open the graphs folder:**

```powershell
start outputs/experiment_v1/graphs
```

**Show each graph and explain:**

#### 5.1 Mean Episode Reward

![](../../outputs/experiment_v1/graphs/mean_episode_reward.png)

> "This shows learning progress. Starting from ~13 reward, it climbs to ~82.
> The steady increase confirms the agent is learning effectively.
> Plateau around step 150K indicates convergence."

#### 5.2 Lambda-2 Reduction

![](../../outputs/experiment_v1/graphs/lambda2_percent_of_initial.png)

> "This is the KEY metric. Î»â‚‚/Î»â‚‚â‚€ represents remaining connectivity.
> Start at 100% (no disruption), drops to ~63.7% (36.3% reduction).
> Below 50% would mean enemy swarm is severely degraded."

#### 5.3 Policy Loss

![](../../outputs/experiment_v1/graphs/policy_loss.png)

> "Policy loss tracks how much the policy is changing.
> Starts noisy (exploration), then stabilizes near zero.
> This is expected behavior for PPO - indicates convergence."

#### 5.4 KL Divergence

![](../../outputs/experiment_v1/graphs/kl_divergence.png)

> "KL divergence measures how different new policy is from old.
> We implemented early stopping when KL > 0.03.
> Notice it stays below threshold - this prevents policy collapse."

---

### Step 6: Explain the Physical Grounding (2 minutes)

**Open `src/physics/fspl.py` and show the formula:**

```python
def received_power_watts(tx_power_watts, distance, frequency, eps=1e-10):
    """
    P_R = P_tx * (c / (4Ï€fd))Â²
    """
    wavelength = SPEED_OF_LIGHT / frequency
    received = tx_power_watts * (wavelength / (4 * np.pi * d)) ** 2
    return received
```

**What to say:**

> "This is the Free-Space Path Loss model from RF textbooks.
> P_R decreases with distance squared and frequency squared.
> This is NOT a made-up formula - it's standard physics.
> Our agents learn to position themselves where this physics works in their favor."

---

### Step 7: Show Band Matching Logic (1 minute)

**Open `src/physics/jamming.py` and show:**

```python
# Only jam if on same frequency band
band_match = (jammer_bands == enemy_band)
jammed = (P_jam >= threshold) & band_match
```

**What to say:**

> "Critical insight: jamming only works if frequencies match.
> Our agents must LEARN to choose the correct band.
> This is why we have a band selection head in the neural network.
> Random band selection gives 25% accuracy; our trained agents achieve much higher."

---

### Step 8: Show Multi-Agent Architecture (2 minutes)

**Open `src/agents/actor.py` and explain:**

```python
# All agents SHARE these weights
self.trunk = nn.Sequential(
    nn.Linear(5, 128),
    nn.LayerNorm(128),
    nn.ReLU(),
    ...
)
```

**What to say:**

> "All 4 jammer agents share the same neural network weights.
> This is called parameter sharing in MARL.
> Advantages:
>
> 1. Faster training (4x samples from same policy)
> 2. Scalability (same weights work for M=4 or M=40)
> 3. Emergent coordination through shared experience"

---

### Step 9: Live Visualization (Optional, 3 minutes)

**If you have visualization script:**

```powershell
python -m src.visualize --experiment_name experiment_v1 --episode_id 0
```

**Shows:**

- Red dots: Enemy positions
- Blue dots: Jammer positions
- Lines: Communication links (gray = intact, red = jammed)
- Animation of jammers moving to disrupt

**What to say:**

> "Watch how the jammers learn to position themselves.
> They move toward enemy clusters and select correct frequencies.
> Links turning red indicate successful jamming.
> This is emergent coordination - no one told them WHERE to go."

---

## ðŸ’¡ Key Points to Emphasize

### Novelty

1. **Physics-grounded RL**: FSPL model, not made-up rewards
2. **Graph-theoretic objective**: Î»â‚‚ as measurable connectivity metric
3. **Multi-band awareness**: Agents learn frequency selection
4. **CTDE architecture**: Centralized critic + decentralized actors

### Technical Depth

1. **PPO with KL early stopping**: Prevents policy collapse
2. **GAE for advantage estimation**: Reduces variance
3. **LayerNorm over BatchNorm**: Works with small M
4. **Parameter sharing**: Scalable to arbitrary M

### Results

1. **36% Î»â‚‚ reduction**: Significant disruption
2. **Stable convergence**: KL < 0.03 throughout
3. **Reproducible**: Same config gives similar results

---

## ðŸ—£ï¸ Hinglish Demo Summary

**Demo dete waqt kya bolna hai:**

"Sir, yeh Multi-Agent Reinforcement Learning project hai jisme 4 jammer drones 10 enemy drones ki communication disrupt karte hain. Physics real hai - FSPL formula use kiya hai RF propagation ke liye, jo communications textbooks mein milta hai.

Main metric hai Lambda-2, jo algebraic connectivity hai graph ki. Agar Î»â‚‚ zero ho jaye toh enemy swarm disconnect ho gaya - wo coordinate nahi kar payenge. Humne 36% reduction achieve kiya hai 200K steps ki training se.

PPO algorithm use kiya hai with parameter sharing - matlab ek hi neural network 4 agents share karte hain. Isse training fast hoti hai aur scalability milti hai. KL divergence early stopping lagaya hai taaki policy collapse na ho.

Band selection bhi agent ko sikhaya hai - wrong frequency pe jamming kaam nahi karti, isliye agents ko sahi band choose karna padta hai. Random choose karein toh 25% accuracy, trained agent zyada accurate hai.

Code modular hai - physics alag, environment alag, agents alag. Industry standard software engineering practices follow ki hain."

---

## â±ï¸ Demo Timing Summary

| Step      | Duration   | Content                       |
| --------- | ---------- | ----------------------------- |
| 1         | 30s        | Project structure             |
| 2         | 30s        | Configuration                 |
| 3         | 5min       | Live training demo            |
| 4         | 2min       | Pre-trained evaluation        |
| 5         | 3min       | Training graphs               |
| 6         | 2min       | FSPL physics                  |
| 7         | 1min       | Band matching                 |
| 8         | 2min       | Multi-agent architecture      |
| 9         | 3min       | Live visualization (optional) |
| **Total** | **~19min** | Full demo                     |

---

**Next:** See `03_CONCEPTS_GLOSSARY.md` for explanation of terms like steps, episodes, rollout, etc.
