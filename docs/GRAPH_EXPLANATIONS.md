# MARL Jammer System - Graph Explanations for Professor Presentation

## Overview

This document provides detailed explanations of each graph generated by the MARL Jammer training system. These graphs are designed to validate the theoretical framework and demonstrate the effectiveness of Multi-Agent Reinforcement Learning (MARL) with Proximal Policy Optimization (PPO) for disrupting enemy drone swarm communication networks.

---

## Graph 1: Lambda-2 (λ₂) vs Training Episodes

### **File:** `1_lambda2_vs_episodes.png`

### **What It Shows:**
The algebraic connectivity (λ₂) of the enemy swarm's communication graph plotted against training progress.

### **Theoretical Background:**
- **Lambda-2 (λ₂)** is the second smallest eigenvalue of the graph Laplacian matrix
- **λ₂ > 0** indicates the network is connected (swarm can communicate)
- **λ₂ = 0** indicates the network is disconnected (swarm is fragmented)
- **Higher λ₂** = stronger connectivity, **Lower λ₂** = weaker connectivity

### **Expected Behavior:**
- **Start (Episode 0):** λ₂ is HIGH (~0.6-0.8 normalized) because jammers start at random positions and haven't learned optimal placement
- **Middle (Training):** λ₂ DECREASES as agents learn to position themselves strategically
- **End (Converged):** λ₂ reaches LOW values (~0.3-0.4 normalized), indicating 60-70% reduction in connectivity

### **Interpretation:**
The decreasing trend proves that:
1. The reward function correctly incentivizes swarm disruption
2. PPO successfully learns the jamming policy
3. The MARL approach effectively fragments the enemy network

### **Key Metrics:**
- **Target Line (Orange, y=0.3):** Represents 70% connectivity reduction
- **Full Disconnection (Red, y=0):** Complete network fragmentation

---

## Graph 2: Reward vs Training Episodes

### **File:** `2_reward_vs_episodes.png`

### **What It Shows:**
The average episode reward plotted against training progress.

### **Theoretical Background:**
The reward function is designed as:
```
R = ω₁·R_λ₂ + ω₂·R_band + ω₃·R_prox + ω₄·R_energy + ω₅·R_overlap
```

Where:
- **R_λ₂:** Lambda-2 reduction reward (primary objective)
- **R_band:** Frequency band matching bonus
- **R_prox:** Proximity to target clusters
- **R_energy:** Energy efficiency penalty
- **R_overlap:** Jammer overlap penalty

### **Expected Behavior:**
- **Start:** LOW reward (~80-100) - agents take random actions
- **Middle:** INCREASING reward as policy improves
- **End:** HIGH reward (~150-180) - converged optimal policy

### **Interpretation:**
An increasing reward curve confirms:
1. The PPO algorithm is learning effectively
2. The policy is improving with more training
3. Convergence is achieved when reward plateaus

### **Key Insight:**
The reward increase correlates with λ₂ decrease, validating that maximizing reward achieves our jamming objective.

---

## Graph 3: Average Received Power (dBm) vs Episodes

### **File:** `3_avg_power_comparison.png`

### **What It Shows:**
Comparison of average jamming power received at enemy communication link midpoints for two approaches:
1. **MARL-PPO (Purple):** Our proposed method
2. **Q-table (Orange):** Traditional tabular Q-learning baseline

### **Theoretical Background:**
- Effective jamming requires delivering sufficient power (> threshold) at the communication link midpoint
- **Higher received power** = more effective jamming
- FSPL (Free Space Path Loss): Power decreases with distance squared

### **Power Levels:**
- **-67 dBm (Threshold):** Minimum power needed for effective jamming
- **-55 dBm (Start):** Typical initial jamming power
- **-43 dBm (Target):** Optimal jamming power after learning

### **Expected Behavior:**
- **MARL-PPO:** Starts at ~-55 dBm, INCREASES to ~-43 dBm (getting closer to targets)
- **Q-table:** Stays FLAT at ~-67 dBm (limited by state-action space explosion)

### **Interpretation:**
The improvement of MARL-PPO over Q-table demonstrates:
1. **Scalability:** PPO handles 100 enemies + 40 jammers where Q-table fails
2. **Continuous optimization:** Smooth positioning vs discrete states
3. **~22 dB improvement:** Significant jamming effectiveness gain

---

## Graph 4: Connectivity Before/After

### **File:** `4_connectivity_before_after.png`

### **What It Shows:**
Side-by-side visualization of the enemy communication graph:
- **LEFT (Before):** Dense connected network with random jammer placement
- **RIGHT (After):** Fragmented network with MARL-optimized jammer placement

### **Visual Elements:**
- **Red dots:** Enemy drones (N=100)
- **Blue/Green triangles:** Jammer drones (M=40)
- **Gray lines:** Communication links between enemies
- **Green circles (Right):** Jammer effective radius

### **Theoretical Background:**
- Before training: Jammers placed randomly, minimal link disruption
- After training: Jammers positioned at cluster centers and communication bridges
- The MARL policy learns to maximize coverage of critical links

### **Expected Behavior:**
- **LEFT:** Many dense communication links visible (high connectivity)
- **RIGHT:** Few sparse links remain (fragmented network)

### **Interpretation:**
This visual proof shows:
1. Strategic jammer placement near enemy clusters
2. Effective disruption of inter-cluster communication bridges
3. Maximum coverage with minimum jammer overlap

---

## Graph 5: Jammer Trajectories

### **File:** `5_jammer_trajectories.png`

### **What It Shows:**
The learned movement trajectories of jammer drones from initial random positions to final optimized positions.

### **Visual Elements:**
- **Open triangles:** Starting positions (random)
- **Filled triangles:** Final positions (optimized)
- **Curved lines:** Movement trajectories during episode
- **Yellow stars:** Enemy cluster centroids
- **Red dots:** Enemy drone positions

### **Theoretical Background:**
The policy network outputs velocity commands:
```
Action = (v_x, v_y, band) for each jammer j ∈ {1,...,M}
```

The trajectories show how the learned policy navigates jammers toward optimal positions.

### **Expected Behavior:**
- Trajectories converge toward enemy cluster centers
- Final positions maximize coverage of communication links
- Minimal overlap between jammers (efficient deployment)

### **Interpretation:**
The trajectory plot demonstrates:
1. **Emergent coordination:** Jammers spread across clusters without explicit communication
2. **Strategic positioning:** Final positions near cluster centroids and bridges
3. **Learned navigation:** Smooth movement from random start to optimal end

---

## Graph 6: Full Dashboard

### **File:** `6_full_dashboard.png`

### **What It Shows:**
A comprehensive 6-panel summary of training results including:
1. λ₂ vs Episodes (core metric)
2. Reward vs Episodes (learning curve)
3. MARL vs Random comparison
4. λ₂ Reduction histogram
5. Policy entropy over time
6. Training summary statistics

### **Purpose:**
Single-page overview for quick assessment of training quality and convergence.

### **Key Indicators of Success:**
- Decreasing λ₂ trend
- Increasing reward trend
- Entropy decay (policy becoming deterministic)
- High mean λ₂ reduction (>50%)

---

## Summary Table

| Graph | Primary Metric | Expected Trend | Success Indicator |
|-------|---------------|----------------|-------------------|
| 1. Lambda-2 | λ₂ (normalized) | ↓ Decreasing | Reaches 0.3-0.4 |
| 2. Reward | Episode return | ↑ Increasing | Reaches 150+ |
| 3. Avg Power | Received dBm | ↑ Increasing | Reaches -45 dBm |
| 4. Connectivity | Visual links | Dense → Sparse | Few links remain |
| 5. Trajectories | Jammer paths | Random → Clustered | Near enemy centers |
| 6. Dashboard | All metrics | All improving | Converged |

---

## How to Generate These Graphs

After training completes, generate all graphs with:

```powershell
python generate_graphs.py --experiment <experiment_name> --graph all --save --no-show
```

Graphs are saved to: `outputs/<experiment_name>/graphs/`

---

## References

- **PROJECT_MASTER_GUIDE_v2.md:** Complete theoretical framework
- **Section 3.5-3.6:** Lambda-2 and graph Laplacian theory
- **Section 5.2:** Reward function design
- **Section 10.1:** Expected graph shapes

---

*Document prepared for academic presentation of MARL Jammer System*
*Last updated: February 2026*
